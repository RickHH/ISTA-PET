import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import scipy.io as sio
import numpy as np
import glob
from time import time
from PIL import Image
import math
import matplotlib.pyplot as plt



cpkt_model_number = 200
n_input = 4096
n_output = 4096
batch_size = 60
PhaseNumber = 9
nrtrain = 420
learning_rate = 0.000001
Epoch_num = 200

print('Load Data')


# load data
H_matrix_path = 'D://ista-net/my_ista/7/H.mat' #H是系统矩阵
H = sio.loadmat(H_matrix_path)['H'].T

Xtrue_path = 'D://ista-net/my_ista/7/X_test.mat'
Xtrue = sio.loadmat(Xtrue_path)['X_test'].T#X是真值图像，也就是label 120*4096

Sinogram_path = 'D://ista-net/my_ista/7/S_test.mat'
sinogram  = sio.loadmat(Sinogram_path)['S_test'].T #sinogram 120*4096




# initialization
X_input = tf.placeholder(tf.float32, [None, n_input])
X_output = tf.placeholder(tf.float32, [None, n_output])

HTH_input = np.dot(H, H.transpose())
H_input = tf.constant(H, dtype=tf.float32)
H_input = tf.transpose(H_input)

X0 = tf.matmul(X_input,H_input)

HTH = tf.constant(HTH_input, dtype=tf.float32)
HTb = tf.matmul(X_input,H_input)






def BN(BNinput,out_size):#Batch normalization
    fc_mean, fc_var = tf.nn.moments(
        BNinput,
        axes=[0,1,2],  # the dimension you wanna normalize, here [0] for batch
        # for image, you wanna do [0, 1, 2] for [batch, height, width] but not channel
    )
    scale = tf.Variable(tf.ones([out_size]))
    shift = tf.Variable(tf.zeros([out_size]))
    epsilon = 0.001

    # apply moving average for mean and var when train on batch
    ema = tf.train.ExponentialMovingAverage(decay=0.5)

    def mean_var_with_update():
        ema_apply_op = ema.apply([fc_mean, fc_var])
        with tf.control_dependencies([ema_apply_op]):
            return tf.identity(fc_mean), tf.identity(fc_var)

    mean, var = mean_var_with_update()

    BNinput = tf.nn.batch_normalization(BNinput, mean, var, shift, scale, epsilon)
    return BNinput

def add_con2d_weight_bias(w_shape, b_shape, order_no):
    Weights = tf.get_variable(shape=w_shape, initializer=tf.contrib.layers.xavier_initializer_conv2d(), name='Weights_%d' % order_no)
    biases = tf.Variable(tf.random_normal(b_shape, stddev=0.05), name='biases_%d' % order_no)
    return [Weights, biases]

def ista_block(input_layers, input_data, layer_no):
    tau_value = tf.Variable(0.1, dtype=tf.float32)
    lambda_step = tf.Variable(0.1, dtype=tf.float32)
    soft_thr = tf.Variable(0.1, dtype=tf.float32)
    conv_size = 32
    filter_size = 3

    x1_ista = tf.add(input_layers[-1] - tf.scalar_mul(lambda_step, tf.matmul(input_layers[-1], HTH)), tf.scalar_mul(lambda_step, HTb))  # X_k - lambda*A^TAX

    x2_ista = tf.reshape(x1_ista, shape=[-1, 64, 64, 1])


    [Weights0, bias0] = add_con2d_weight_bias([filter_size, filter_size, 1, conv_size], [conv_size], 0)

    [Weights1, bias1] = add_con2d_weight_bias([filter_size, filter_size, conv_size, conv_size], [conv_size], 1)
    [Weights11, bias11] = add_con2d_weight_bias([filter_size, filter_size, conv_size, conv_size], [conv_size], 11)

    [Weights2, bias2] = add_con2d_weight_bias([filter_size, filter_size, conv_size, conv_size], [conv_size], 2)
    [Weights22, bias22] = add_con2d_weight_bias([filter_size, filter_size, conv_size, conv_size], [conv_size], 22)


    [Weights3, bias3] = add_con2d_weight_bias([filter_size, filter_size, conv_size, 1], [1], 3)


    x3_ista = tf.nn.conv2d(x2_ista, Weights0, strides=[1, 1, 1, 1], padding='SAME')
    x3_ista = BN(x3_ista,np.size(x3_ista))#BN x3

    x4_ista = tf.nn.relu(tf.nn.conv2d(x3_ista, Weights1, strides=[1, 1, 1, 1], padding='SAME'))
    x44_ista = tf.nn.conv2d(x4_ista, Weights11, strides=[1, 1, 1, 1], padding='SAME')
    x44_ista = BN(x44_ista, np.size(x44_ista))

    x5_ista = tf.multiply(tf.sign(x44_ista), tf.nn.relu(tf.abs(x44_ista) - soft_thr))
    x5_ista = BN(x5_ista, np.size(x5_ista))

    x6_ista = tf.nn.relu(tf.nn.conv2d(x5_ista, Weights2, strides=[1, 1, 1, 1], padding='SAME'))
    x66_ista = tf.nn.conv2d(x6_ista, Weights22, strides=[1, 1, 1, 1], padding='SAME')
    x66_ista = BN(x66_ista, np.size(x66_ista))

    x7_ista = tf.nn.conv2d(x66_ista, Weights3, strides=[1, 1, 1, 1], padding='SAME')

    x7_ista = x7_ista + x2_ista
    x7_ista = BN(x7_ista, np.size(x7_ista))

    x8_ista = tf.reshape(x7_ista, shape=[-1, 4096])

    x3_ista_sym = tf.nn.relu(tf.nn.conv2d(x3_ista, Weights1, strides=[1, 1, 1, 1], padding='SAME'))
    x4_ista_sym = tf.nn.conv2d(x3_ista_sym, Weights11, strides=[1, 1, 1, 1], padding='SAME')
    x4_ista_sym = BN(x4_ista_sym,np.size(x4_ista_sym))
    x6_ista_sym = tf.nn.relu(tf.nn.conv2d(x4_ista_sym, Weights2, strides=[1, 1, 1, 1], padding='SAME'))
    x7_ista_sym = tf.nn.conv2d(x6_ista_sym, Weights22, strides=[1, 1, 1, 1], padding='SAME')
    x7_ista_sym = BN(x7_ista_sym, np.size(x7_ista_sym))

    x11_ista = x7_ista_sym - x3_ista

    return [x8_ista, x11_ista]


def inference_ista(input_tensor, n, X_output, reuse):
    layers = []
    layers_symetric = []
    layers.append(input_tensor)
    for i in range(n):
        with tf.variable_scope('conv_%d' %i, reuse=reuse):
            [conv1, conv1_sym] = ista_block(layers, X_output, i)
            layers.append(conv1)
            layers_symetric.append(conv1_sym)
    return [layers, layers_symetric]


def compute_cost(Prediction, X_output, PhaseNumber):
    cost = tf.reduce_mean(tf.square(Prediction[-1] - X_output))
    cost_sym = 0
    for k in range(PhaseNumber):
        cost_sym += tf.reduce_mean(tf.square(Pre_symetric[k]))

    return [cost, cost_sym]



[Prediction, Pre_symetric] = inference_ista(X0, PhaseNumber, X_output, reuse=False)

cost0 = tf.reduce_mean(tf.square(X0 - X_output))

[cost, cost_sym] = compute_cost(Prediction, X_output, PhaseNumber)

cost_all = cost + 0.01*cost_sym


optm_all = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_all)

init = tf.global_variables_initializer()




config = tf.ConfigProto()
config.gpu_options.allow_growth = True

saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)
sess = tf.Session(config=config)

# load model
model_dir = 'Phase_%d_ISTA_Net_plus_Model4' % PhaseNumber
saver.restore(sess, './%s/CS_Saved_Model4_%d.cpkt' % (model_dir, cpkt_model_number))

# reconstruction
Img_input = sinogram
start = time()
Prediction_value = sess.run(Prediction[-1], feed_dict={X_input: Img_input})
X_initialize = sess.run(X0, feed_dict = {X_input: Img_input})
end = time()


print(X_initialize.shape)
plt.imshow(X_initialize[1,:].reshape(64,64).T)
plt.show()


print(Prediction_value.shape)
plt.imshow(Prediction_value[1,:].reshape(64,64).T)
plt.show()

X11=Xtrue[1,:]
X22=X11.reshape(64,64)
plt.imshow(X22.T)
plt.show()

# data_init = 'D://ista-net/result/init_brain1.mat.'
#
# sio.savemat(data_init, {'X_i':X_initialize})
#
# data_recons = 'D://ista-net/result/recons_brain1.mat.'
#
# sio.savemat(data_recons, {'X_r':Prediction_value})
