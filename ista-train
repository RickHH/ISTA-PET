#!/usr/bin/env python
# coding: utf-8

# In[1]:


import tensorflow as tf
import scipy.io as sio
import numpy as np
import glob
from time import time
from PIL import Image
import math
import matplotlib
import matplotlib.pyplot as plt
import os




# In[2]:
# os.environ["CUDA_VISIBLE_DEVICES"] = "0"

cpkt_model_number = 2000
n_input = 4096
n_output = 4096
batch_size = 60
PhaseNumber = 9
nrtrain = 420
learning_rate = 0.000001
Epoch_num = 2000

print('Load Data')


# load H matrix
H_matrix_path = '/home/hr/my_ista/7/H.mat' 
H = sio.loadmat(H_matrix_path)['H'].T
# load S
Xtrue_path = '/home/hr/my_ista/7/X_train.mat'
Xtrue = sio.loadmat(Xtrue_path)['X_train'].T

Sinogram_path = '/home/hr/my_ista/7/S_train.mat'
sinogram  = sio.loadmat(Sinogram_path)['S_train'].T 



X_input = tf.placeholder(tf.float32, [None, n_input])
X_output = tf.placeholder(tf.float32, [None, n_output])

HTH_input = np.dot(H, H.transpose())
H_input = tf.constant(H, dtype=tf.float32)
H_input = tf.transpose(H_input)


X0 = tf.matmul(X_input,H_input)

HTH = tf.constant(HTH_input, dtype=tf.float32)
HTb = tf.matmul(X_input,H_input)


# In[5]:


def add_con2d_weight_bias(w_shape, b_shape, order_no):
    Weights = tf.get_variable(shape=w_shape, initializer=tf.contrib.layers.xavier_initializer_conv2d(), name='Weights_%d' % order_no)
    biases = tf.Variable(tf.random_normal(b_shape, stddev=0.05), name='biases_%d' % order_no)
    return [Weights, biases]
def BN(BNinput,out_size):#Batch normalization
    fc_mean, fc_var = tf.nn.moments(
        BNinput,
        axes=[0,1,2],  # the dimension you wanna normalize, here [0] for batch
        # for image, you wanna do [0, 1, 2] for [batch, height, width] but not channel
    )
    scale = tf.Variable(tf.ones([out_size]))
    shift = tf.Variable(tf.zeros([out_size]))
    epsilon = 0.001

    # apply moving average for mean and var when train on batch
    ema = tf.train.ExponentialMovingAverage(decay=0.5)

    def mean_var_with_update():
        ema_apply_op = ema.apply([fc_mean, fc_var])
        with tf.control_dependencies([ema_apply_op]):
            return tf.identity(fc_mean), tf.identity(fc_var)

    mean, var = mean_var_with_update()

    BNinput = tf.nn.batch_normalization(BNinput, mean, var, shift, scale, epsilon)
    return BNinput
    # similar with this two steps:
    # Wx_plus_b = (Wx_plus_b - fc_mean) / tf.sqrt(fc_var + 0.001)
    # Wx_plus_b = Wx_plus_b * scale + shift
def ista_block(input_layers, input_data, layer_no):
    tau_value = tf.Variable(0.1, dtype=tf.float32)
    lambda_step = tf.Variable(0.1, dtype=tf.float32)
    soft_thr = tf.Variable(0.1, dtype=tf.float32)
    conv_size = 32
    filter_size = 3

    x1_ista = tf.add(input_layers[-1] - tf.scalar_mul(lambda_step, tf.matmul(input_layers[-1], HTH)), tf.scalar_mul(lambda_step, HTb))  # X_k - lambda*A^TAX

    x2_ista = tf.reshape(x1_ista, shape=[-1, 64, 64, 1])


    [Weights0, bias0] = add_con2d_weight_bias([filter_size, filter_size, 1, conv_size], [conv_size], 0)

    [Weights1, bias1] = add_con2d_weight_bias([filter_size, filter_size, conv_size, conv_size], [conv_size], 1)
    [Weights11, bias11] = add_con2d_weight_bias([filter_size, filter_size, conv_size, conv_size], [conv_size], 11)

    [Weights2, bias2] = add_con2d_weight_bias([filter_size, filter_size, conv_size, conv_size], [conv_size], 2)
    [Weights22, bias22] = add_con2d_weight_bias([filter_size, filter_size, conv_size, conv_size], [conv_size], 22)


    [Weights3, bias3] = add_con2d_weight_bias([filter_size, filter_size, conv_size, 1], [1], 3)


    x3_ista = tf.nn.conv2d(x2_ista, Weights0, strides=[1, 1, 1, 1], padding='SAME')
    x3_ista = BN(x3_ista,np.size(x3_ista))#BN x3

    x4_ista = tf.nn.relu(tf.nn.conv2d(x3_ista, Weights1, strides=[1, 1, 1, 1], padding='SAME'))
    x44_ista = tf.nn.conv2d(x4_ista, Weights11, strides=[1, 1, 1, 1], padding='SAME')
    x44_ista = BN(x44_ista, np.size(x44_ista))

    x5_ista = tf.multiply(tf.sign(x44_ista), tf.nn.relu(tf.abs(x44_ista) - soft_thr))
    x5_ista = BN(x5_ista, np.size(x5_ista))

    x6_ista = tf.nn.relu(tf.nn.conv2d(x5_ista, Weights2, strides=[1, 1, 1, 1], padding='SAME'))
    x66_ista = tf.nn.conv2d(x6_ista, Weights22, strides=[1, 1, 1, 1], padding='SAME')
    x66_ista = BN(x66_ista, np.size(x66_ista))

    x7_ista = tf.nn.conv2d(x66_ista, Weights3, strides=[1, 1, 1, 1], padding='SAME')

    x7_ista = x7_ista + x2_ista
    x7_ista = BN(x7_ista, np.size(x7_ista))

    x8_ista = tf.reshape(x7_ista, shape=[-1, 4096])

    x3_ista_sym = tf.nn.relu(tf.nn.conv2d(x3_ista, Weights1, strides=[1, 1, 1, 1], padding='SAME'))
    x4_ista_sym = tf.nn.conv2d(x3_ista_sym, Weights11, strides=[1, 1, 1, 1], padding='SAME')
    x4_ista_sym = BN(x4_ista_sym,np.size(x4_ista_sym))
    x6_ista_sym = tf.nn.relu(tf.nn.conv2d(x4_ista_sym, Weights2, strides=[1, 1, 1, 1], padding='SAME'))
    x7_ista_sym = tf.nn.conv2d(x6_ista_sym, Weights22, strides=[1, 1, 1, 1], padding='SAME')
    x7_ista_sym = BN(x7_ista_sym, np.size(x7_ista_sym))

    x11_ista = x7_ista_sym - x3_ista

    return [x8_ista, x11_ista]
def inference_ista(input_tensor, n, X_output, reuse):
    layers = []
    layers_symetric = []
    layers.append(input_tensor)
    for i in range(n):
        with tf.variable_scope('conv_%d' %i, reuse=reuse):
            [conv1, conv1_sym] = ista_block(layers, X_output, i)
            layers.append(conv1)
            layers_symetric.append(conv1_sym)
    return [layers, layers_symetric]
[Prediction, Pre_symetric] = inference_ista(X0, PhaseNumber, X_output, reuse=False)
cost0 = tf.reduce_mean(tf.square(X0 - X_output))
# In[6]:
def compute_cost(Prediction, X_output, PhaseNumber):
    cost = tf.reduce_mean(tf.square(Prediction[-1] - X_output))
    cost_sym = 0
    for k in range(PhaseNumber):
        cost_sym += tf.reduce_mean(tf.square(Pre_symetric[k]))

    return [cost, cost_sym]
[cost, cost_sym] = compute_cost(Prediction, X_output, PhaseNumber)
cost_all = cost + 0.01*cost_sym
optm_all = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_all)
# optm_all = tf.train.MomentumOptimizer(learning_rate=0.00001,momentum=0.5,use_nesterov=True).minimize(cost_all)
init = tf.global_variables_initializer()
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)
# In[7]:
sess = tf.Session(config=config)
sess.run(init)

print("...............................")
print("Phase Number is %d" % PhaseNumber)
print("...............................\n")

print("Strart Training..")


model_dir = 'Phase_%d_ISTA_Net_plus_Model4' % PhaseNumber

output_file_name = "Log_output_%s.txt" % (model_dir)

for epoch_i in range(0, Epoch_num+1):
    randidx_all = np.random.permutation(nrtrain)
    for batch_i in range(nrtrain // batch_size):
        randidx = randidx_all[batch_i*batch_size:(batch_i+1)*batch_size]

        batch_xs = sinogram[randidx, :]
        batch_ys = Xtrue[randidx, :]

        feed_dict = {X_input: batch_xs, X_output: batch_ys}
        sess.run(optm_all, feed_dict=feed_dict)

    output_data = "[%02d/%02d] cost: %.4f, cost_sym: %.4f \n" % (epoch_i, Epoch_num, sess.run(cost, feed_dict=feed_dict), sess.run(cost_sym, feed_dict=feed_dict))
    print(output_data)

    output_file = open(output_file_name, 'a')
    output_file.write(output_data)
    output_file.close()

    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    if epoch_i <= 100:
        saver.save(sess, './%s/CS_Saved_Model4_%d.cpkt' % (model_dir, epoch_i), write_meta_graph=False)
    else:
        if epoch_i % 100 == 0:
            saver.save(sess, './%s/CS_Saved_Model4_%d.cpkt' % (model_dir, epoch_i), write_meta_graph=False)


print("Training Finished")
sess.close()


# In[ ]:




